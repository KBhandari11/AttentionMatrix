{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import GPT2Model,GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import imageio.v2 as imageio\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "\n",
    "from utils.plot_trajectories import plot_trajectory, plot_trajectory_random\n",
    "from utils.get_embedding import generate_embeddings\n",
    "from utils.get_network_details import get_unweighted,compute_network_distribution_property "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "few_shot_example_prompts = [\"Question: What is the sum of 5 and 6?\\n Answer: 11\",\"Question: What is the sum of 1 and 9?\\n Answer: 10\", \"Question: What is the sum of 0 and 0?\\n Answer: 0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt2-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-xl'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(few_shot_example_prompts[0], return_tensors='pt')\n",
    "_,_, _, generated_text = generate_embeddings(inputs,model,tokenizer)\n",
    "print(few_shot_example_prompts[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2-xl'\n",
    "model = GPT2Model.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.3)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.2)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.1)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.3)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.2)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.1)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.3)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.2)\n",
    "plot_trajectory(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.1)\n",
    "plot_trajectory_random(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.4)\n",
    "plot_trajectory_random(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.3)\n",
    "plot_trajectory_random(model,tokenizer,model_name,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model_name_llama = 'meta-llama/Llama-3.1-8B'\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False)\n",
    "plot_trajectory_random(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.3)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.2)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=0, svd=False, threshold=0.1)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.3)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.2)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=2, svd=False, threshold=0.1)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.3)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.2)\n",
    "plot_trajectory(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.1)\n",
    "plot_trajectory_random(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.4)\n",
    "plot_trajectory_random(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.3)\n",
    "plot_trajectory_random(model_llama,tokenizer_llama,model_name_llama,[(10,13),(\"a\", \"b\"),(12315645616,1231564516816184)],few_shot_example_prompts,few_shot=3, svd=False, threshold=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
